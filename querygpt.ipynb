{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCqqa/wfNFWos7d3gwM2RA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkishore/machinelearning/blob/main/querygpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###import library and key"
      ],
      "metadata": {
        "id": "kjXeli0qpgyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install tiktoken\n",
        "!pip install chromadb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import langchain\n",
        "langchain.debug=False\n",
        "import os, openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_=load_dotenv(find_dotenv())\n",
        "openai.api_key=os.environ['API_KEY']\n"
      ],
      "metadata": {
        "id": "-xeZPJ3NnAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and split"
      ],
      "metadata": {
        "id": "V1eDmh94qbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "\n",
        "#Load\n",
        "loader=TextLoader('/content/sample1.txt')\n",
        "docs=loader.load()\n",
        "\n",
        "#Split\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "texts=text_splitter.split_documents(docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "5IUUNx23q0ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM embedding and storing in VectorDB"
      ],
      "metadata": {
        "id": "CEjmpDMrj-BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = '/content/chroma/'\n",
        "\n",
        "# to remove old database files\n",
        "# !rm -rf ./content/chroma/\n",
        "\n",
        "db=Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "V2nRl5dVkLzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###retrieval from VectorDB/ Through LLM"
      ],
      "metadata": {
        "id": "wiwzmt3pvDZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #fetching relevant data directly from vector db using similarilty search\n",
        "# Query='Is entity id required for all kinds of applications?'\n",
        "# # result=db.similarity_search(Query, k=2)\n",
        "# result1=db.max_marginal_relevance_search(Query,fetch_k=4,k=3)\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0,openai_api_key= openai.api_key)\n",
        "\n",
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def run_model(question):\n",
        "  qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=db.as_retriever(search_type='mmr',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}, verbose =True)\n",
        "  result=qa_chain({\"query\":question})\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "k2lNuo7rkL-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"what is step 3 in SAML integration process?\")"
      ],
      "metadata": {
        "id": "qIkggFm1gr6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Memory store inclusion in the flow"
      ],
      "metadata": {
        "id": "LowBvIN3iVcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# including memory to store past conversation\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "def run_memory_chain(question):\n",
        "  qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(search_type='similarity',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "    chain_type=\"stuff\",\n",
        "    memory=memory,\n",
        "    verbose=True)\n",
        "\n",
        "  result =qa({\"question\":question})\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "rVK0EKpqNy9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_memory_chain(\"what is step 3 in SAML integration process?\")\n",
        "\n",
        "\n",
        "# question = \"why is he like this?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# question = \"shall we all be like him?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# memory.clear()"
      ],
      "metadata": {
        "id": "Fq-LQ2YpJwWq",
        "outputId": "086d3065-3d51-4585-f8ea-0bb09d2450e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Basic SAML Configuration settings:\n",
            "\n",
            "Step 3. Manage the SAML signing certificate\n",
            "Azure AD uses a certificate to sign the SAML tokens it sends to the application. ESO and the Azure Migration team can help you to view or download the active certificate. If there is a need to update, create, or import a certificate please fill out relevant RITMs with the ESO team.\n",
            "\n",
            "For gallery applications, details about the certificate format are available in the applicationâ€™s SAML documentation (see the application-specific tutorials).\n",
            "\n",
            "Step 2. Edit Basic SAML Configurations Create a RITM with the ESO Team\n",
            "Fill out relevant RITMs with the ESO team to configure the Basic SAML configurations for single sign-on. Follow the steps here.\n",
            "\n",
            "Be sure to identify any custom claims that are required by the application. This can include claims such as Enterprise ID and People Key. These custom claims must be explicitly requested in the RITM.\n",
            "Human: what is step 3 in SAML integration process?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is step 3 in SAML integration process?',\n",
              " 'chat_history': [HumanMessage(content='what is step 3 in SAML integration process?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.', additional_kwargs={}, example=False)],\n",
              " 'answer': 'Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.'}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory.clear()\n",
        "memory"
      ],
      "metadata": {
        "id": "JIcQelP1fQhv",
        "outputId": "aa817294-d7ec-4e4a-abc5-2c38b7e8272a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "df1y34gUVzMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Azure OpenAI Key"
      ],
      "metadata": {
        "id": "sHEfc4Yfr7Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Simple API call"
      ],
      "metadata": {
        "id": "x5Z3pwkXjq_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_=load_dotenv(find_dotenv())\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=os.environ.get(\"ENDPOINT\"),\n",
        "    api_key=os.environ.get(\"API_KEY\"),\n",
        "    api_version=\"2024-05-01-preview\",\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=os.environ.get(\"Deployment_Name\"),\n",
        "    messages= [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are funny bot answering questions with joke\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"\n",
        "    }],\n",
        "    max_tokens=500,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=None,\n",
        "    stream=False\n",
        "\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsjj3cfS3Yon",
        "outputId": "4c22d43f-93c6-430e-dd51-4aa5924ab29f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the Azure Machine Learning and Azure AI services go to different parties? \n",
            "\n",
            "Because one wanted to learn the algorithms, while the other just wanted to have a good time with artificial intelligence!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using Langchain for simple API call"
      ],
      "metadata": {
        "id": "USzEArWIjt5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade langchain\n",
        "# !pip install langchain_community\n",
        "# !pip install -U langchain-openai\n",
        "\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# # Load environment variables from the .env file\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# # Retrieve Azure OpenAI specific configuration from environment variables\n",
        "OPENAI_API_KEY = os.getenv(\"API_KEY\")\n",
        "OPENAI_API_BASE = os.getenv(\"ENDPOINT\")\n",
        "OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
        "Deployment_Name=os.getenv(\"Deployment_Name\")\n",
        "\n",
        "\n",
        "# Initialize an instance of AzureChatOpenAI using the specified settings\n",
        "chat_llm = AzureChatOpenAI(\n",
        "    api_version=OPENAI_API_VERSION,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    azure_endpoint=OPENAI_API_BASE,\n",
        "    openai_api_type=\"azure\",\n",
        "    azure_deployment=Deployment_Name\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are funny bot answering questions with joke\"},\n",
        "    {\"role\": \"user\", \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}\n",
        "]\n",
        "messages1= \"Who are you?\"\n",
        "\n",
        "result=chat_llm.invoke(messages)\n",
        "result\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT7kDHK8shTv",
        "outputId": "72067653-7845-476d-c9c1-a62fcc6e27c3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the Azure Machine Learning and Azure AI services start dating? Because they're a perfect match! \n",
            "\n",
            "But in all seriousness, let me break it down for you: \n",
            "\n",
            "Azure Machine Learning is a cloud-based platform that allows developers and data scientists to build, deploy, and manage machine learning models. It provides tools and infrastructure to train, test, and deploy models at scale.\n",
            "\n",
            "On the other hand, Azure AI services are a collection of pre-built AI capabilities offered by Microsoft. These services include computer vision, natural language processing, speech recognition, and more. They are designed to make it easier for developers to add AI capabilities to their applications without having to build everything from scratch.\n",
            "\n",
            "So, while Azure Machine Learning focuses on the end-to-end machine learning process, Azure AI services provide ready-to-use AI capabilities that can be integrated into various applications. They complement each other, like two peas in a pod!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AzureOpenAI ChatLLM Response: \", chat_llm.predict(\"you are funny bot, tell me about big bang theory\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piadnkko3g44",
        "outputId": "b6accd46-3024-49ba-9c98-04ec40606232"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AzureOpenAI ChatLLM Response:  Thank you! I'm glad you find me funny. As for \"The Big Bang Theory,\" it's a popular American sitcom that aired from 2007 to 2019. The show revolves around a group of socially awkward but highly intelligent scientists, Leonard, Sheldon, Howard, and Raj, along with their neighbor Penny. The series humorously explores their personal lives, friendships, romantic relationships, and their adventures in the world of science. It's known for its witty dialogue, geeky references, and iconic catchphrases. If you're a fan of nerdy humor and pop culture references, you might enjoy watching it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using Langchain for vector Indexing"
      ],
      "metadata": {
        "id": "gsChoeAdlgum"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7_DEBUQ3g7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjmJtjuj3g-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}