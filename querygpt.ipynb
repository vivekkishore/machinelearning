{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMrDU0LAV+c4LjMGSby1g7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkishore/machinelearning/blob/main/querygpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###import library and key"
      ],
      "metadata": {
        "id": "kjXeli0qpgyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install tiktoken\n",
        "!pip install chromadb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import langchain\n",
        "langchain.debug=False\n",
        "import os, openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_=load_dotenv(find_dotenv())\n",
        "openai.api_key=os.environ['API_KEY']\n"
      ],
      "metadata": {
        "id": "-xeZPJ3NnAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and split"
      ],
      "metadata": {
        "id": "V1eDmh94qbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "\n",
        "#Load\n",
        "loader=TextLoader('/content/sample1.txt')\n",
        "docs=loader.load()\n",
        "\n",
        "#Split\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "texts=text_splitter.split_documents(docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "5IUUNx23q0ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM embedding and storing in VectorDB"
      ],
      "metadata": {
        "id": "CEjmpDMrj-BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = '/content/chroma/'\n",
        "\n",
        "# to remove old database files\n",
        "# !rm -rf ./content/chroma/\n",
        "\n",
        "db=Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "V2nRl5dVkLzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###retrieval from VectorDB/ Through LLM"
      ],
      "metadata": {
        "id": "wiwzmt3pvDZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #fetching relevant data directly from vector db using similarilty search\n",
        "# Query='Is entity id required for all kinds of applications?'\n",
        "# # result=db.similarity_search(Query, k=2)\n",
        "# result1=db.max_marginal_relevance_search(Query,fetch_k=4,k=3)\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0,openai_api_key= openai.api_key)\n",
        "\n",
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def run_model(question):\n",
        "  qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=db.as_retriever(search_type='mmr',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}, verbose =True)\n",
        "  result=qa_chain({\"query\":question})\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "k2lNuo7rkL-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"what is step 3 in SAML integration process?\")"
      ],
      "metadata": {
        "id": "qIkggFm1gr6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Memory store inclusion in the flow"
      ],
      "metadata": {
        "id": "LowBvIN3iVcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# including memory to store past conversation\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "def run_memory_chain(question):\n",
        "  qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(search_type='similarity',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "    chain_type=\"stuff\",\n",
        "    memory=memory,\n",
        "    verbose=True)\n",
        "\n",
        "  result =qa({\"question\":question})\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "rVK0EKpqNy9Y"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_memory_chain(\"what is step 3 in SAML integration process?\")\n",
        "\n",
        "\n",
        "# question = \"why is he like this?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# question = \"shall we all be like him?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# memory.clear()"
      ],
      "metadata": {
        "id": "Fq-LQ2YpJwWq",
        "outputId": "086d3065-3d51-4585-f8ea-0bb09d2450e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Basic SAML Configuration settings:\n",
            "\n",
            "Step 3. Manage the SAML signing certificate\n",
            "Azure AD uses a certificate to sign the SAML tokens it sends to the application. ESO and the Azure Migration team can help you to view or download the active certificate. If there is a need to update, create, or import a certificate please fill out relevant RITMs with the ESO team.\n",
            "\n",
            "For gallery applications, details about the certificate format are available in the applicationâ€™s SAML documentation (see the application-specific tutorials).\n",
            "\n",
            "Step 2. Edit Basic SAML Configurations Create a RITM with the ESO Team\n",
            "Fill out relevant RITMs with the ESO team to configure the Basic SAML configurations for single sign-on. Follow the steps here.\n",
            "\n",
            "Be sure to identify any custom claims that are required by the application. This can include claims such as Enterprise ID and People Key. These custom claims must be explicitly requested in the RITM.\n",
            "Human: what is step 3 in SAML integration process?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is step 3 in SAML integration process?',\n",
              " 'chat_history': [HumanMessage(content='what is step 3 in SAML integration process?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.', additional_kwargs={}, example=False)],\n",
              " 'answer': 'Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.'}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory.clear()\n",
        "memory"
      ],
      "metadata": {
        "id": "JIcQelP1fQhv",
        "outputId": "aa817294-d7ec-4e4a-abc5-2c38b7e8272a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "df1y34gUVzMR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqIjwIv3xNnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}