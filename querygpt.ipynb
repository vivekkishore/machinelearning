{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi4khv3/WET6ilY3jpj3DU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkishore/machinelearning/blob/main/querygpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###import library and key"
      ],
      "metadata": {
        "id": "kjXeli0qpgyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install tiktoken\n",
        "!pip install chromadb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import langchain\n",
        "langchain.debug=False\n",
        "import os, openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_=load_dotenv(find_dotenv())\n",
        "openai.api_key=os.environ['API_KEY']\n"
      ],
      "metadata": {
        "id": "-xeZPJ3NnAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and split"
      ],
      "metadata": {
        "id": "V1eDmh94qbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "\n",
        "#Load\n",
        "loader=TextLoader('/content/sample1.txt')\n",
        "docs=loader.load()\n",
        "\n",
        "#Split\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "texts=text_splitter.split_documents(docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "5IUUNx23q0ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM embedding and storing in VectorDB"
      ],
      "metadata": {
        "id": "CEjmpDMrj-BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = '/content/chroma/'\n",
        "\n",
        "# to remove old database files\n",
        "# !rm -rf ./content/chroma/\n",
        "\n",
        "db=Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "V2nRl5dVkLzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###retrieval from VectorDB/ Through LLM"
      ],
      "metadata": {
        "id": "wiwzmt3pvDZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #fetching relevant data directly from vector db using similarilty search\n",
        "# Query='Is entity id required for all kinds of applications?'\n",
        "# # result=db.similarity_search(Query, k=2)\n",
        "# result1=db.max_marginal_relevance_search(Query,fetch_k=4,k=3)\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0,openai_api_key= openai.api_key)\n",
        "\n",
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def run_model(question):\n",
        "  qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=db.as_retriever(search_type='mmr',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}, verbose =True)\n",
        "  result=qa_chain({\"query\":question})\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "k2lNuo7rkL-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"what is step 3 in SAML integration process?\")"
      ],
      "metadata": {
        "id": "qIkggFm1gr6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Memory store inclusion in the flow"
      ],
      "metadata": {
        "id": "LowBvIN3iVcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# including memory to store past conversation\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "def run_memory_chain(question):\n",
        "  qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(search_type='similarity',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "    chain_type=\"stuff\",\n",
        "    memory=memory,\n",
        "    verbose=True)\n",
        "\n",
        "  result =qa({\"question\":question})\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "rVK0EKpqNy9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_memory_chain(\"what is step 3 in SAML integration process?\")\n",
        "\n",
        "\n",
        "# question = \"why is he like this?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# question = \"shall we all be like him?\"\n",
        "# result = qa({\"question\": question})\n",
        "# result\n",
        "\n",
        "# memory.clear()"
      ],
      "metadata": {
        "id": "Fq-LQ2YpJwWq",
        "outputId": "086d3065-3d51-4585-f8ea-0bb09d2450e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Basic SAML Configuration settings:\n",
            "\n",
            "Step 3. Manage the SAML signing certificate\n",
            "Azure AD uses a certificate to sign the SAML tokens it sends to the application. ESO and the Azure Migration team can help you to view or download the active certificate. If there is a need to update, create, or import a certificate please fill out relevant RITMs with the ESO team.\n",
            "\n",
            "For gallery applications, details about the certificate format are available in the applicationâ€™s SAML documentation (see the application-specific tutorials).\n",
            "\n",
            "Step 2. Edit Basic SAML Configurations Create a RITM with the ESO Team\n",
            "Fill out relevant RITMs with the ESO team to configure the Basic SAML configurations for single sign-on. Follow the steps here.\n",
            "\n",
            "Be sure to identify any custom claims that are required by the application. This can include claims such as Enterprise ID and People Key. These custom claims must be explicitly requested in the RITM.\n",
            "Human: what is step 3 in SAML integration process?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is step 3 in SAML integration process?',\n",
              " 'chat_history': [HumanMessage(content='what is step 3 in SAML integration process?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.', additional_kwargs={}, example=False)],\n",
              " 'answer': 'Step 3 in the SAML integration process is to manage the SAML signing certificate. This involves using a certificate to sign the SAML tokens that Azure AD sends to the application. You can view or download the active certificate with the help of the ESO team. If there is a need to update, create, or import a certificate, you should fill out relevant RITMs with the ESO team.'}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory.clear()\n",
        "memory"
      ],
      "metadata": {
        "id": "JIcQelP1fQhv",
        "outputId": "aa817294-d7ec-4e4a-abc5-2c38b7e8272a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "df1y34gUVzMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Azure OpenAI Key"
      ],
      "metadata": {
        "id": "sHEfc4Yfr7Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Simple API call"
      ],
      "metadata": {
        "id": "x5Z3pwkXjq_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_=load_dotenv(find_dotenv())\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=os.environ.get(\"ENDPOINT\"),\n",
        "    api_key=os.environ.get(\"API_KEY\"),\n",
        "    api_version=\"2024-05-01-preview\",\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=os.environ.get(\"Deployment_Name\"),\n",
        "    messages= [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are funny bot answering questions with joke\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"\n",
        "    }],\n",
        "    max_tokens=500,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=None,\n",
        "    stream=False\n",
        "\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsjj3cfS3Yon",
        "outputId": "4c22d43f-93c6-430e-dd51-4aa5924ab29f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the Azure Machine Learning and Azure AI services go to different parties? \n",
            "\n",
            "Because one wanted to learn the algorithms, while the other just wanted to have a good time with artificial intelligence!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using Langchain for simple API call"
      ],
      "metadata": {
        "id": "USzEArWIjt5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai\n",
        "!pip install python-dotenv\n",
        "\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# # Load environment variables from the .env file\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# # Retrieve Azure OpenAI specific configuration from environment variables\n",
        "OPENAI_API_KEY = os.getenv(\"API_KEY\")\n",
        "OPENAI_API_BASE = os.getenv(\"ENDPOINT\")\n",
        "OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
        "Deployment_Name=os.getenv(\"Deployment_Name\")\n",
        "\n",
        "\n",
        "# Initialize an instance of AzureChatOpenAI using the specified settings\n",
        "chat_llm = AzureChatOpenAI(\n",
        "    api_version=OPENAI_API_VERSION,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    azure_endpoint=OPENAI_API_BASE,\n",
        "    openai_api_type=\"azure\",\n",
        "    azure_deployment=Deployment_Name\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are funny bot answering questions with joke\"},\n",
        "    {\"role\": \"user\", \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}\n",
        "]\n",
        "messages1= \"Who are you?\"\n",
        "\n",
        "result=chat_llm.invoke(messages)\n",
        "result\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT7kDHK8shTv",
        "outputId": "8ec63456-d497-4b75-a910-246e8af71571"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Why did the AI go to the gym? \n",
            "\n",
            "Because it wanted some byte-sized exercise! \n",
            "\n",
            "But in all seriousness, Azure Machine Learning is a platform that allows you to build, deploy, and manage machine learning models. It provides tools and services for data scientists to develop and experiment with their models.\n",
            "\n",
            "On the other hand, Azure AI services are a collection of pre-built AI capabilities that can be easily integrated into your applications. These services include computer vision, natural language processing, speech recognition, and more. They are designed to provide ready-to-use AI functionality without the need for extensive machine learning expertise.\n",
            "\n",
            "So, in a nutshell, Azure Machine Learning is more focused on building and training models, while Azure AI services offer out-of-the-box AI capabilities for various tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AzureOpenAI ChatLLM Response: \", chat_llm.predict(\"you are funny bot, tell me about big bang theory\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piadnkko3g44",
        "outputId": "b6accd46-3024-49ba-9c98-04ec40606232"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AzureOpenAI ChatLLM Response:  Thank you! I'm glad you find me funny. As for \"The Big Bang Theory,\" it's a popular American sitcom that aired from 2007 to 2019. The show revolves around a group of socially awkward but highly intelligent scientists, Leonard, Sheldon, Howard, and Raj, along with their neighbor Penny. The series humorously explores their personal lives, friendships, romantic relationships, and their adventures in the world of science. It's known for its witty dialogue, geeky references, and iconic catchphrases. If you're a fan of nerdy humor and pop culture references, you might enjoy watching it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using Langchain for vector Indexing"
      ],
      "metadata": {
        "id": "gsChoeAdlgum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code to query a dataset using Azure openai key and langchain\n",
        "\n",
        "# ###Using Langchain for vector Indexing\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "# Retrieve Azure OpenAI specific configuration from environment variables\n",
        "OPENAI_API_KEY = os.getenv(\"API_KEY\")\n",
        "OPENAI_API_BASE = os.getenv(\"ENDPOINT\")\n",
        "OPENAI_API_VERSION = \"2024-05-01-preview\"\n",
        "Deployment_Name=os.getenv(\"Deployment_Name\")\n",
        "\n",
        "# Initialize Azure OpenAI embeddings\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    deployment=Deployment_Name,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    openai_api_base=OPENAI_API_BASE,\n",
        "    openai_api_version=OPENAI_API_VERSION,\n",
        "    chunk_size=1\n",
        ")\n",
        "\n",
        "# Load documents\n",
        "loader=TextLoader('/content/sample1.txt')\n",
        "docs=loader.load()\n",
        "\n",
        "# Split documents\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "texts=text_splitter.split_documents(docs)\n",
        "\n",
        "# Create Chroma database\n",
        "persist_directory = '/content/chroma/'\n",
        "db=Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\n",
        "\n",
        "# Initialize AzureChatOpenAI LLM\n",
        "chat_llm = AzureChatOpenAI(\n",
        "    api_version=OPENAI_API_VERSION,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    azure_endpoint=OPENAI_API_BASE,\n",
        "    openai_api_type=\"azure\",\n",
        "    azure_deployment=Deployment_Name\n",
        ")\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
        "\n",
        "# Run chain\n",
        "def run_model(question):\n",
        "  qa_chain = RetrievalQA.from_chain_type(chat_llm,\n",
        "                                       retriever=db.as_retriever(search_type='mmr',search_kwargs={'k': 3, 'fetch_k':4 }),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}, verbose =True)\n",
        "  result=qa_chain({\"query\":question})\n",
        "  return result\n",
        "\n",
        "# Query the dataset\n",
        "run_model(\"what is step 3 in SAML integration process?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7_DEBUQ3g7P",
        "outputId": "a36afb9e-6927-4d25-eae6-25880d4483f5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.messages.ai.AIMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = chat_llm\n",
        "\n",
        "llm_prompt = PromptTemplate(\n",
        "    input_variables=[\"human_prompt\"],\n",
        "    template=\"The following is a conversation with an AI assistant. The assistant is helpful.\\n\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: {human_prompt}?\",\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=llm_prompt)\n",
        "\n",
        "prompt=\"Wht is your name?\"\n",
        "return chain.run(prompt) # prompt is human input from request body"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "WjmJtjuj3g-k",
        "outputId": "d9281bdd-a9a8-419a-e9e5-844ce2cedf52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chat_llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a116435211f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m llm_prompt = PromptTemplate(\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"human_prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The following is a conversation with an AI assistant. The assistant is helpful.\\n\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: {human_prompt}?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chat_llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkWvTIYBGzN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phlkZjUGGzQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ogqJ2cwGzTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cODPBKBJGzWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4x7b2cLiGzZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}