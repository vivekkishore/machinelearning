{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpa1tVpYJqm/qTYiXfpIxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkishore/machinelearning/blob/main/llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLama index"
      ],
      "metadata": {
        "id": "0qDz3eH74MKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query GPT with LLama index"
      ],
      "metadata": {
        "id": "rxI1PrhiKwli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index\n",
        "# !pip install llama-index-llms-azure-openai\n",
        "# !pip install python-dotenv"
      ],
      "metadata": {
        "id": "Dea_T-YJK1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureOpenAI(\n",
        "    model=azure_gpt_model_name,\n",
        "    deployment_name=gpt_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")\n",
        "response = llm.complete(\"The sky is blue\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd0NdpUgK2Bh",
        "outputId": "9c43d42e-dbfd-4c7b-d548-957442b34ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, that is correct. The sky appears blue during the day due to the scattering of sunlight by the Earth's atmosphere.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages=[\n",
        "  ChatMessage(role='system',content='You are funny and helpful bot'\n",
        "    ),\n",
        "    ChatMessage(role='user',content='why is sky blue?')\n",
        "]\n",
        "\n",
        "response=llm.chat(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikzm5A6JK2Es",
        "outputId": "823fbe3c-7455-4269-f81a-21694a5b55e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Well, the sky is blue because it's trying to be cool and stand out from all the other colors. It's like the sky's way of saying, \"Hey, look at me, I'm the coolest color around!\" Plus, blue is just a really calming and peaceful color, so it's the perfect choice for the sky.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using LLama index to query list of JSON"
      ],
      "metadata": {
        "id": "WlgNLgs4DICc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF2mJWTtg8td"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-index\n",
        "# !pip install llama-index-llms-azure-openai\n",
        "# !pip install llama-index-embeddings-azure-openai\n",
        "# !pip install sqlite-utils\n",
        "# !pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\n",
        "    {'name':'charls','age':10,'city':'Newyork'},\n",
        "    {'name':'bob','age':15,'city':'delhi'},\n",
        "    {'name':'alice','age':20,'city':'mumbai'},\n",
        "     {'name':'john','age':25,'city':'chicago'},\n",
        "     {'name':'maddie','age':21,'city':'pune'}\n",
        "     ]"
      ],
      "metadata": {
        "id": "lAMizLp94KLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from dotenv import dotenv_values\n",
        "config = dotenv_values(\".env\")\n",
        "\n",
        "\n",
        "azure_openai_key = config[\"API_KEY\"]\n",
        "azure_openai_endpoint = config[\"ENDPOINT\"]\n",
        "azure_gpt_model_name=config[\"GPT_Model_Name\"]\n",
        "gpt_deployment_name=config[\"Deployment_Name\"]\n",
        "api_version=config['Model_Version']\n",
        "azure_embed_model_name=config[\"Embed_Model_Name\"]\n",
        "embedding_deployment_name=config[\"Embed_Deployment\"]\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=azure_gpt_model_name,\n",
        "    deployment_name=gpt_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")\n",
        "\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=azure_embed_model_name,\n",
        "    deployment_name=embedding_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")"
      ],
      "metadata": {
        "id": "3o0B9vJk4KOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import JSONalyzeQueryEngine\n",
        "#JSON data is being converted to SQL first\n",
        "engine=JSONalyzeQueryEngine(\n",
        "    list_of_dict=data,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Rk3x6pq3FwcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=engine.query(\"Do anyone live in city bangalore?\")\n",
        "result"
      ],
      "metadata": {
        "id": "pAujX9VZFwe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9b9f46-0700-4248-e49c-c70ead58dc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;32mQuery: Do anyone live in city bangalore?\n",
            "\u001b[0m\u001b[1;3;34mSQL Query: SELECT COUNT(*) FROM items WHERE city = 'bangalore'\n",
            "\u001b[0m\u001b[1;3;36mTable Schema: {'name': <class 'str'>, 'age': <class 'int'>, 'city': <class 'str'>}\n",
            "\u001b[0m\u001b[1;3;33mSQL Response: [{'COUNT(*)': 0}]\n",
            "\u001b[0m\u001b[1;3;35mResponse: No, no one lives in the city of Bangalore.\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='No, no one lives in the city of Bangalore.', source_nodes=[], metadata={'sql_query': \"SELECT COUNT(*) FROM items WHERE city = 'bangalore'\", 'table_schema': \"{'name': <class 'str'>, 'age': <class 'int'>, 'city': <class 'str'>}\"})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_HgpU7wIPkc",
        "outputId": "a37da6c6-cc2e-41fb-da34-1fd87bf1c13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, no one lives in the city of Bangalore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query text file with LLamma index\n",
        "\n"
      ],
      "metadata": {
        "id": "eHWPBu1VM4WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index docx2txt pypdf python-dotenv llama-index-embeddings-azure-openai llama-index-llms-azure-openai"
      ],
      "metadata": {
        "id": "b8zArcawFwgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "config=dotenv_values(\"val.env\")\n",
        "\n",
        "azure_openai_key = config[\"API_KEY\"]\n",
        "azure_openai_endpoint = config[\"ENDPOINT\"]\n",
        "azure_gpt_model_name=config[\"GPT_Model_Name\"]\n",
        "gpt_deployment_name=config[\"Deployment_Name\"]\n",
        "api_version=config['Model_Version']\n",
        "azure_embed_model_name=config[\"Embed_Model_Name\"]\n",
        "embedding_deployment_name=config[\"Embed_Deployment\"]\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=azure_gpt_model_name,\n",
        "    deployment_name=gpt_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")\n",
        "\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=azure_embed_model_name,\n",
        "    deployment_name=embedding_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")"
      ],
      "metadata": {
        "id": "lyZ6v74fFwkc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "VFb2foSm9_Sc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex,StorageContext\n",
        "\n",
        "documents = SimpleDirectoryReader( input_files=[\"sample1.txt\"]).load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "oXfPrw1qiVNB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is SP initiated integration?\"\n",
        "query_engine=index.as_query_engine(llm=llm )\n",
        "answer=query_engine.query(query)\n",
        "\n",
        "# print(answer.get_formatted_sources())\n",
        "print(\"query was:\", query)\n",
        "print(\"answer is:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMFRAWHfNlcR",
        "outputId": "8fa595d6-9627-4c7c-e7c7-06451bbb260f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query was: What is SP initiated integration?\n",
            "answer is: SP initiated integration refers to a type of integration where the service provider (SP) takes the initiative to establish a connection or interaction with another system or service. In this type of integration, the SP is responsible for initiating the communication and sending the necessary data or requests to the other system. This can be done through various protocols and mechanisms, such as API calls, webhooks, or direct messaging. SP initiated integration allows the SP to control the flow of information and initiate actions based on specific triggers or events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "template=(\n",
        "    \"I have provided the context below \\n\"\n",
        "    \"-------------------------\\n\"\n",
        "    \"You are a funny bot who always starts the answer with one joke \\n\"\n",
        "    \"---------------------------------- \\n\"\n",
        "    \"Given this context, please answer this question: {query_str} \\n\"\n",
        ")\n",
        "\n",
        "query=\"What is SP initiated integration?\"\n",
        "qa_prompt=PromptTemplate(template)\n",
        "\n",
        "prompt=qa_prompt.format(query_str=query)\n",
        "# print(prompt)\n",
        "\n",
        "answer=query_engine.query(prompt)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "r3MCFHBVKqE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561f526c-08f8-4ca3-ffab-491b74aa9dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the scarecrow win an award? Because he was outstanding in his field! \n",
            "\n",
            "SP-initiated integration is a login process in which a user navigates to the vendor application, which then redirects the user to Azure AD for authentication. After authentication, the user is redirected back to the application with an authorization token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Using PDF, store index and reload"
      ],
      "metadata": {
        "id": "FNwVVad8Mgpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents1 = SimpleDirectoryReader( input_files=[\"newfile1.pdf\"]).load_data()\n",
        "index1 = VectorStoreIndex.from_documents(documents1)\n",
        "\n",
        "# Specify the directory to persist the index\n",
        "persist_dir = \"./index_storage\"\n",
        "\n",
        "# Persist the index to the specified directory\n",
        "index1.storage_context.persist(persist_dir=persist_dir)"
      ],
      "metadata": {
        "id": "JGWdk12rKlex"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import load_index_from_storage\n",
        "import os\n",
        "\n",
        "# Check if the index storage directory exists\n",
        "if os.path.exists(persist_dir):\n",
        "    # Rebuild the storage context with the directory where the index was persisted\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "\n",
        "    # Load the index from storage\n",
        "    index2 = load_index_from_storage(storage_context)\n",
        "else:\n",
        "    print(\"Index storage not found. Please check the directory path.\")"
      ],
      "metadata": {
        "id": "szxxA5xjOyCp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1=\"what all information this document provide?\"\n",
        "query_engine1=index2.as_query_engine()\n",
        "answer=query_engine1.query(query1)\n",
        "\n",
        "# print(answer.get_formatted_sources())\n",
        "print(\"query was:\", query1)\n",
        "print(\"answer is:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk4kTx3_Kljh",
        "outputId": "7db193a1-84fd-4b91-8bc8-7b854b92a4fb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query was: what all information this document provide?\n",
            "answer is: This document provides the text of the Hanuman Chalisa in English, along with a description in English. It describes the qualities and attributes of Hanuman, such as his strength, wisdom, and devotion to Lord Ram. It also mentions the benefits of reciting the Hanuman Chalisa, such as protection from evil forces and the fulfillment of desires.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query pdf with LLama index and pyMuPDF"
      ],
      "metadata": {
        "id": "-PGr0BSbPhSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index docx2txt pypdf python-dotenv llama-index-embeddings-azure-openai llama-index-llms-azure-openai\n",
        "!pip install fitz\n",
        "!pip install frontend\n",
        "# !pip install PyMuPDF"
      ],
      "metadata": {
        "id": "WcMLZ3kYPgN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "config=dotenv_values(\"val.env\")\n",
        "\n",
        "azure_openai_key = config[\"API_KEY\"]\n",
        "azure_openai_endpoint = config[\"ENDPOINT\"]\n",
        "azure_gpt_model_name=config[\"GPT_Model_Name\"]\n",
        "gpt_deployment_name=config[\"Deployment_Name\"]\n",
        "api_version=config['Model_Version']\n",
        "azure_embed_model_name=config[\"Embed_Model_Name\"]\n",
        "embedding_deployment_name=config[\"Embed_Deployment\"]\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=azure_gpt_model_name,\n",
        "    deployment_name=gpt_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")\n",
        "\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=azure_embed_model_name,\n",
        "    deployment_name=embedding_deployment_name,\n",
        "    api_key=azure_openai_key,\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_version=api_version,\n",
        ")\n",
        "\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "lUD9pE7DPgQn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "# Load your document\n",
        "loader = PyMuPDFReader()\n",
        "documents = loader.load(file_path=\"/content/newfile.pdf\")\n",
        "\n",
        "# Create a vector store index\n",
        "service_context = ServiceContext.from_defaults()\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "CK5akAvsPgUF",
        "outputId": "f3a1064e-5d74-4add-f13a-e9602da7fbd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Directory 'static/' does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7b5e705270f8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load your document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyMuPDFReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/newfile.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a vector store index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/readers/file/pymu_pdf/base.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file_path, metadata, extra_info)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# check if file_path is a string or Path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fitz/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfrontend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0.0.1dev2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/events/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclipboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevent_mixins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhash_change\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/events/clipboard.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevent_mixins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClipboardDataMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ClipboardEvent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/dom.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/dispatcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstarlette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebsockets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0masync_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlater_await\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/frontend/server.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStarlette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATIC_ROUTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStaticFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATIC_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATIC_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_middleware\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGZipMiddleware\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m app.add_middleware(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/starlette/staticfiles.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, packages, html, check_dir, follow_symlink)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollow_symlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfollow_symlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_dir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory '{directory}' does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     def get_directories(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Directory 'static/' does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbiCpaFibYyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmgzW_lXbY18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}