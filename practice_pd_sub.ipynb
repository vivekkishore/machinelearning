{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR3RVZ+wMzMsfJW61TtM2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkishore/machinelearning/blob/main/practice_pd_sub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4XaWkTO5nwn"
      },
      "outputs": [],
      "source": [
        "from abc import ABC,abstractmethod\n",
        "class shape(ABC):\n",
        "  def __init__(self,shapetype,color):\n",
        "    self.shapetype=shapetype\n",
        "    self.color=color\n",
        "\n",
        "  @abstractmethod\n",
        "  def area(self):\n",
        "    pass\n",
        "  def get_color(self):\n",
        "    return self.color\n",
        "  def get_shapetype(self):\n",
        "    return self.shapetype\n",
        "\n",
        "class square(shape):\n",
        "  def __init__(self,side,color):\n",
        "    self.side=side\n",
        "    super().__init__('square',color)\n",
        "  def area(self):\n",
        "    return self.side*self.side\n",
        "\n",
        "class circle(shape):\n",
        "  def __init__(self,radius,color):\n",
        "    self.radius=radius\n",
        "    super().__init__('circle',color)\n",
        "  def area(self):\n",
        "    return 3.14*self.radius*self.radius\n",
        "\n",
        "class rectangle(shape):\n",
        "  def __init__(self,length,width,color):\n",
        "    self.length=length\n",
        "    self.width=width\n",
        "    super().__init__('rectangle',color)\n",
        "  def area(self):\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sq1=square(5,'red')\n",
        "print(sq1.area())\n",
        "print(sq1.get_color())\n",
        "print(sq1.get_shapetype())"
      ],
      "metadata": {
        "id": "64TFtFhN5unp",
        "outputId": "bf2ab1f0-8149-4071-f005-c4999488fc9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "red\n",
            "square\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Agent and worflows"
      ],
      "metadata": {
        "id": "FpqCLhLsovnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -qU langchain-groq\n",
        "# from google.colab import userdata\n",
        "# import os\n",
        "# from langchain_groq import ChatGroq\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# from pprint import pprint\n",
        "# from IPython.display import display, Markdown\n",
        "\n",
        "# os.environ['GROQ_API_KEY']=userdata.get('groq_key')\n",
        "\n",
        "# model=ChatGroq(model='llama-3.3-70b-versatile')\n",
        "\n",
        "# basic_prompt = \"\"\"\n",
        "# You are a knowledgeable AI assistant specializing in trend analysis and reporting.\n",
        "# Please address the following inquiry:\n",
        "# Query: {query}\n",
        "# \"\"\"\n",
        "\n",
        "# template_obj = PromptTemplate(\n",
        "#     input_variables=[\"query\"],\n",
        "#     template=basic_prompt\n",
        "# )\n",
        "\n",
        "# llmchain= template_obj | model | StrOutputParser()\n",
        "result=llmchain.invoke({'query':'How does AI affect the global healthcare landscape?'})\n",
        "\n",
        "print(f\"Agent reply: {result}\")\n"
      ],
      "metadata": {
        "id": "AWsJ_V5M5ur3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzjqxFDYynun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0mD1N2SaZ7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlu0AhO9aZ31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEjBnB6baZ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessage(\"Translate the following from English into Italian\"),\n",
        "#     HumanMessage(\"hi!\"),\n",
        "# ]\n",
        "# model.invoke(messages)\n",
        "\n",
        "# from groq import Groq\n",
        "\n",
        "# client = Groq()\n",
        "# completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.3-70b-versatile\",\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"who are you?\"\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_tokens=1024,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# print(completion.choices[0].message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QrNhrMEtPzC",
        "outputId": "0e407c9a-3ade-4aff-82cb-2fe92490fb63"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvfcwBtOtPp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9JX_mQivWss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ax8j3aEwvWpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsYFGyjVvWnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}